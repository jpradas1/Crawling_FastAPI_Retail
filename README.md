# Crawling Engineer FastAPI

This repository builds up an API service to consume data generated by the scraper [Crawling_Engineer_Challenge](https://github.com/jpradas1/Crawling_Engineer_Challenge), which extracts data about thousand of available products in [Adidas](https://www.adidas.es/) and [Puma](https://eu.puma.com/).

The folder [dataset](https://github.com/jpradas1/Crawling_FastAPI_retail/tree/main/dataset) contains json files to created the MongoDB database.

## Creating MongoDB Database
The python script `mongo.py` automatically creates the database using those json files, although the MongoClient needs the url server.
### Local
We can create a local MongoDB server using Docker.
```
sudo docker pull mongo
sudo docker run -d -p 27017:27017 --name mongodb mongo
```
And just run the python script
```
python3 mongo.py
```
Hence we create a database named as **products** with a collection **items**, each document is a product from the stores.
### Cloud
If you have a Mongo server on cloud you need the url connection and save it as a environment variable.
```
export MONGO_URL="mongodb://{MONGOUSER}:{MONGOPASSWORD}@{MONGOHOST}:{MONGOPORT}"
```
or for Windows
```
set MONGO_URL="mongodb://{MONGOUSER}:{MONGOPASSWORD}@{MONGOHOST}:{MONGOPORT}"
```
Then modify `mongo.py` in order to stablish connection to Mongo Server on Cloud.

## Run FastAPI Locally
To run this project locally using whatever Mongo server, just run

### Mongo Local Server
```
MONGO_URL="mongodb://localhost:27017/" uvicorn main:app --reload
```

### Mongo Cloud Server
```
MONGO_URL="mongodb://{MONGOUSER}:{MONGOPASSWORD}@{MONGOHOST}:{MONGOPORT}" uvicorn main:app --reload
```
And start to consume data on `localhost:8000/` and documentation on `localhost:8000/docs`.

## Deploy
There's a deploy of this API using [render.com](https://render.com/). To reach it out:
### [Crawling Engineer FastAPI](https://crawling-engineer-fastapi.onrender.com/docs)
